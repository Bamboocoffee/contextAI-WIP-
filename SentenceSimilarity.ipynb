{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Stephen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Stephen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Stephen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Stephen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Stephen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Stephen/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from word_parsing import tokenize, tag_tokens\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "def displayHeadlines(source, headlines):\n",
    "\n",
    "    source = []\n",
    "    count = 0\n",
    "    for head in headlines:\n",
    "        if count < 5:\n",
    "            source.append(head.getText())\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return source\n",
    "\n",
    "def journalScrape(top_headlines):\n",
    "\n",
    "    r1 = requests.get('https://www.thejournal.ie/irish')\n",
    "    coverpage = r1.content\n",
    "\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "    headlines = soup1.find_all('div', class_='text span-5 last')\n",
    "\n",
    "    complete_headlines = []\n",
    "\n",
    "    for tag in headlines:\n",
    "        atag = tag.find_all('h4', class_=None)\n",
    "        for a in atag:\n",
    "            text = a.find_all('a')\n",
    "            headline = displayHeadlines(\"Journal\", text)\n",
    "            complete_headlines += headline\n",
    "\n",
    "    top_headlines[\"Journal\"] = complete_headlines[0:5]\n",
    "    return top_headlines\n",
    "\n",
    "def RTEScrape(top_headlines):\n",
    "\n",
    "    r1 = requests.get('https://www.rte.ie/news')\n",
    "    coverpage = r1.content\n",
    "\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "    headlines = soup1.find_all('span', class_='underline')\n",
    "\n",
    "    headlines = displayHeadlines(\"RTE\", headlines)\n",
    "    top_headlines[\"RTE\"] = headlines\n",
    "    return top_headlines\n",
    "\n",
    "def irishIndependantScrape(top_headlines):\n",
    "    r1 = requests.get('https://www.independent.ie')\n",
    "    coverpage = r1.content\n",
    "\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "    headlines = soup1.find_all('h3', class_='hx')\n",
    "\n",
    "    headlines = displayHeadlines(\"Independant\", headlines)\n",
    "    top_headlines[\"Independant\"] = headlines\n",
    "    return top_headlines\n",
    "\n",
    "def irishTimesScrape(top_headlines):\n",
    "    r1 = requests.get('https://www.irishtimes.com/')\n",
    "    coverpage = r1.content\n",
    "\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "    headlines = soup1.find_all('span', class_='tr-headline')\n",
    "\n",
    "    headlines = displayHeadlines(\"Irish Times\", headlines)\n",
    "    top_headlines[\"Irish Times\"] = headlines\n",
    "    return top_headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def irishSites():\n",
    "    top_headlines = dict()\n",
    "    top_headlines = irishTimesScrape(top_headlines)\n",
    "    top_headlines = irishIndependantScrape(top_headlines)\n",
    "    top_headlines = RTEScrape(top_headlines)\n",
    "    top_headlines = journalScrape(top_headlines)\n",
    "    \n",
    "    return top_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "def tokenize_stop_lemmatize(data):\n",
    "    \"\"\" Check if data structure is a dictionary.\n",
    "    Tokensize the words in each of the titles.\n",
    "    Extract the Nouns and Verbs and remove stop words\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuations=\"?:!.,;\"\n",
    "\n",
    "    if data == dict():\n",
    "        for source, articles in data.items():\n",
    "            token_articles = []\n",
    "            for article in articles:\n",
    "                    token_article = word_tokenize(article)\n",
    "                    filtered_article = [w for w in token_article if (not w in stop_words and not w in punctuations)]\n",
    "                    filtered_article = [w.lower() for w in filtered_article]\n",
    "                    token_articles.append(filtered_article)\n",
    "\n",
    "            data[source] = token_articles\n",
    "\n",
    "        return data\n",
    "\n",
    "    else:\n",
    "        token_articles = []\n",
    "\n",
    "        for article in data:\n",
    "            token_article = word_tokenize(article)\n",
    "            filtered_article = [w.lower() for w in token_article if (not w in stop_words and not w in punctuations)]\n",
    "            token_articles.append(filtered_article)\n",
    "\n",
    "        return token_articles\n",
    "\n",
    "def getStringArticles(dictionary):\n",
    "\n",
    "    article_list = []\n",
    "\n",
    "    for articles in dictionary.values():\n",
    "        for article in articles:\n",
    "            article = (article.replace('\\n', ''))\n",
    "            article_list.append(article)\n",
    "\n",
    "    return article_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['budget', '2020', 'the', 'main', 'points'], ['brexit', 'johnson', 'allies', 'blame', 'merkel', 'admit', 'deal', 'hopes', 'effectively', 'dead'], ['budget', '2020', 'donohoe', 'allocates', '€1.2bn', 'tackle', 'no-deal', 'brexit'], ['michelin', 'guide', '2020', 'full', 'list', 'irish', 'restaurants', 'judges', 'said'], ['boy', 'cut', 'eyebrow', 'fall', 'dublin', 'crèche', 'awarded', '€32,500'], ['breaking', 'minister', 'announces', '€1.2', 'billion', 'package', 'respond', 'no-deal', 'brexit'], ['live', 'the', 'brexit', 'budget', 'donohoe', 'presents', 'budget', '2020', 'dáil'], ['breaking', 'budget', '2020', 'boost', 'parents', 'early', 'learning', 'childcare', 'gets', '€54m'], ['exclusive', \"'eight\", '10', 'people', 'staying', 'one', 'bedroom', \"'\", '-', 'secret', 'footage', 'reveals', 'asylum', 'seekers', 'crammed', 'hotel'], [\"'it\", \"'s\", 'vanished', 'face', 'earth', \"'\", '-', 'son', \"'s\", 'appeal', 'help', 'finding', 'missing', 'mum'], ['watch', 'paschal', 'donohoe', \"'s\", 'budget', 'speech'], ['brexit', 'pressing', 'risk', 'economy', 'says', 'donohoe'], ['explained', 'the', 'background', 'budget', '2020'], ['source', 'reveals', \"'sensational\", \"'\", 'line', 'brexit', 'talks'], ['tusk', 'accuses', 'johnson', 'playing', 'brexit', \"'blame\", 'game', \"'\"], ['gardaí', 'warn', '27', '%', 'burglars', 'come', 'front', 'door', 'officers', 'launch', 'winter', 'operation'], ['a', 'pack', '20', 'cigarettes', 'cost', '€13.50'], ['live', 'paschal', 'donohoe', 'unveiling', \"'absolutely\", 'surprises', \"'\", 'budget', '2020'], ['free', 'dental', 'care', 'under-6s', 'free', 'gp', 'visits', 'extended', 'under-8s', 'september', '2020'], ['budget', '2020', 'here', 'main', 'points', 'need', 'know']]\n"
     ]
    }
   ],
   "source": [
    "top_headlines = irishSites()\n",
    "string_articles = getStringArticles(top_headlines)\n",
    "token = tokenize_stop_lemmatize(string_articles)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We are now going to try and evaluate the topics that are coveredin the headlines. (Topic Modeling)\n",
    "2. We can then look at which artilces have the topics. (Comparison)\n",
    "3. Compile a list of the articles that have the topics. (Aggregation)\n",
    "4. Run analysis on the articles to invesitgate which is the most unbiased. (Further Research Needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://datascience.stackexchange.com/questions/23969/sentence-similarity-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/Stephen/data/word2vec/GoogleNews-vectors-negative300.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-633a109a18d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mPATH_TO_GLOVE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/data/glove/glove.840B.300d.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_WORD2VEC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/Stephen/data/word2vec/GoogleNews-vectors-negative300.bin'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "PATH_TO_WORD2VEC = os.path.expanduser(\"~/data/word2vec/GoogleNews-vectors-negative300.bin\")\n",
    "PATH_TO_GLOVE = os.path.expanduser(\"~/data/glove/glove.840B.300d.txt\")\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(PATH_TO_WORD2VEC, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tag_tokens(dictionary):\n",
    "#     \"\"\" Compare the returned words.\n",
    "#     If they match with any of them, increment that article by 1 score.\n",
    "#     Order them by the scoring.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         for source, articles in dictionary.items():\n",
    "#             tagged_tokens = []\n",
    "#             for i in articles:\n",
    "#                 i = [v.lower() for v in i]\n",
    "#                 i = pos_tag(i)\n",
    "#                 tagged_tokens.append(i)\n",
    "\n",
    "#             dictionary[source] = tagged_tokens\n",
    "\n",
    "#         return dictionary\n",
    "\n",
    "#     except ValueError:\n",
    "#         return \"Error in data structure. \\\n",
    "#         Please see the input data type and ensure that the dictionary is Source : Articles[]. \\\n",
    "#         Then ensure that the values are tokenized\"\n",
    "\n",
    "# def getNouns(dictionary):\n",
    "#     \"\"\" Remove all tokens that are not Nouns and return the dictionary\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         for source, articles in dictionary.items():\n",
    "#             clean_articles = []\n",
    "#             for tokens in articles:\n",
    "#                 nouns = []\n",
    "#                 for token in tokens:\n",
    "#                     if token[1] == \"NN\" or token[1] == \"NNS\" or token[1] == \"VBG\":\n",
    "#                         nouns.append(token)\n",
    "\n",
    "#                 clean_articles.append(nouns)\n",
    "\n",
    "#             dictionary[source] = clean_articles\n",
    "\n",
    "#         return dictionary\n",
    "\n",
    "#     except:\n",
    "#         return \"Error, the nouns could not be identified. Check the articles\"\n",
    "\n",
    "# def sentenceSimilarities(data):\n",
    "    \n",
    "#     embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "\n",
    "#     tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "#     with tf.Session() as session:\n",
    "#         session.run([tf.global_variables_initialiser(), tf.tables_initializer()])\n",
    "#         embeddings = session.run(embed(data))\n",
    "\n",
    "#     return np.array(embeddings).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict, Counter\n",
    "\n",
    "# def getOccurances(dictionary):\n",
    "\n",
    "#     dict_word_count = defaultdict(int)\n",
    "\n",
    "#     for source, articles in dictionary.items():\n",
    "#         for article in articles:\n",
    "#             for word in article:\n",
    "#                 dict_word_count[word[0]] += 1\n",
    "\n",
    "#     return dict_word_count\n",
    "\n",
    "# def getStringArticles(dictionary):\n",
    "\n",
    "#     article_list = []\n",
    "\n",
    "#     for articles in dictionary.values():\n",
    "#         for article in articles:\n",
    "#             article = (article.replace('\\n', ''))\n",
    "#             article_list.append(article)\n",
    "            \n",
    "#     return article_list\n",
    "\n",
    "# def compareArticles(dictionary):\n",
    "#     pass\n",
    "#     # list of articles that are tokenize\n",
    "#     #\n",
    "#     # for each item in the list compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
